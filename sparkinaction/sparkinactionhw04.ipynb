{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sparkinactionhw04\n",
    "1）以HDFS中文件为输入，做word count操作，观察各Stage的Data locality，并截图。  \n",
    "2）将HDFS中文件读为RDD，转化为Java集合，并通过broadcast广播到各个executor。通过parallelize方法，创建一个只有一个partition的RDD，对该RDD使用flatMap方法，在flatMap方法中读取广播变量（Java集合），然后作word count操作，观察此时各stage的data locality，并截图  \n",
    "3）按照视频的步骤，验证Stage间的串行执行，及Stage内的并行执行，以及Task内的pipeline执行。截图证明  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1）以HDFS中文件为输入，做word count操作，观察各Stage的Data locality，并截图。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成SparkSession实例\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[*]\") \\\n",
    "     .appName(\"Word Count\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过sparkSession获取上下文\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"/test.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rdd1.flatMap(lambda x:x.split()).map(lambda x:(x,1)).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paramMap.update({lr.regParam:', 1),\n",
       " ('#', 20),\n",
       " ('Specify', 2),\n",
       " ('multiple', 1),\n",
       " ('Params.', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共有两个stage  \n",
    "![image.png](https://upload-images.jianshu.io/upload_images/2850424-ad7685f38b768215.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "第一个stage的data-locality  \n",
    "![image.png](https://upload-images.jianshu.io/upload_images/2850424-9ff04ac76228c705.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "第二个stage的data-locality  \n",
    "![image.png](https://upload-images.jianshu.io/upload_images/2850424-882bb983e2ee2440.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2）将HDFS中文件读为RDD，转化为Java集合，并通过broadcast广播到各个executor。通过parallelize方法，创建一个只有一个partition的RDD，对该RDD使用flatMap方法，在flatMap方法中读取广播变量（Java集合），然后作word count操作，观察此时各stage的data locality，并截图 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过sparkSession获取上下文\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.textFile(\"/test.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastValues = sc.broadcast(rdd1.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parti = sc.parallelize(broadcastValues.value,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('from', 4),\n",
       " ('pyspark.ml.linalg', 1),\n",
       " ('import', 3),\n",
       " ('Vectors', 1),\n",
       " ('pyspark.ml.classification', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = parti.flatMap(lambda x:x.split()).map(lambda x:(x,1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "r1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个stage的data-locality  \n",
    "![image.png](https://upload-images.jianshu.io/upload_images/2850424-4589e3ccbedb2c1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)  \n",
    "第二个stage的data-locality  \n",
    "![image.png](https://upload-images.jianshu.io/upload_images/2850424-40215f0b375db680.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3）按照视频的步骤，验证Stage间的串行执行，及Stage内的并行执行，以及Task内的pipeline执行。截图证明 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "认真的看了老师视频中的操作，深刻理解了“Stage间的串行执行，及Stage内的并行执行，以及Task内的pipeline执行”的运行过程。  \n",
    "但是由于我用的是python，所用IDE无法进行单步调试，故无法进行这个过程。望见谅！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
