{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kerashw09\n",
    "完成预测程序。  \n",
    "--------\n",
    "在老师给出的cut_word(s)基础上进行了小改。  \n",
    "老师的程序，如果预测的末尾有分割符号的话如：人们常说生活是一部教科书。，会报错：\n",
    ">IndexError: index 0 is out of bounds for axis 0 with size 0\n",
    "\n",
    "这些课程很有收获。谢谢！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "print(\"load model\")\n",
    "model = load_model('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 128)           660864    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 64)            98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 32, 5)             325       \n",
      "=================================================================\n",
      "Total params: 760,005\n",
      "Trainable params: 760,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置参数\n",
    "# 词向量长度\n",
    "word_size = 128\n",
    "# 设置最长的一句话为32个字\n",
    "maxlen = 32\n",
    "# 批次大小\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用全数据\n",
    "text = open('msr_train.txt').read()\n",
    "text = text.split('\\n')\n",
    "\n",
    "# 根据符号分句\n",
    "text = u''.join(text)\n",
    "text = re.split(u'[，。！？、]/[bems]', text)\n",
    "\n",
    "# 训练集数据\n",
    "data = []\n",
    "# 标签\n",
    "label = []\n",
    "\n",
    "# 得到所有的数据和标签\n",
    "def get_data(s):\n",
    "    s = re.findall('(.)/(.)', s)\n",
    "    if s:\n",
    "        s = np.array(s)\n",
    "        # 返回数据和标签，0为数据，1为标签\n",
    "        return list(s[:,0]), list(s[:,1])\n",
    "\n",
    "for s in text:\n",
    "    d = get_data(s)\n",
    "    if d:\n",
    "        data.append(d[0])\n",
    "        label.append(d[1])\n",
    "        \n",
    "# 定义一个dataframe存放数据和标签\n",
    "d = pd.DataFrame(index=range(len(data)))\n",
    "d['data'] = data\n",
    "d['label'] = label\n",
    "# 提取data长度小于等于maxlen的数据\n",
    "d = d[d['data'].apply(len) <= maxlen]\n",
    "# 重新排列index\n",
    "d.index = range(len(d))\n",
    "\n",
    "#统计所有字，给每个字编号\n",
    "chars = [] \n",
    "for i in data:\n",
    "    chars.extend(i)\n",
    "\n",
    "chars = pd.Series(chars).value_counts()\n",
    "chars[:] = range(1, len(chars)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sb': 412351, 'be': 949884, 'es': 378288, 'ss': 266154, 'bm': 183969, 'me': 183974, 'mm': 170711, 'eb': 528062}\n"
     ]
    }
   ],
   "source": [
    "# 统计状态转移\n",
    "dict_label = {}\n",
    "for label in d['label']:\n",
    "    for i in range(len(label)-1):\n",
    "        tag = label[i] + label[i+1]\n",
    "        dict_label[tag] = dict_label.get(tag,0) + 1\n",
    "print(dict_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3073393"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算状态转移总次数\n",
    "sum_num = 0\n",
    "for value in dict_label.values():\n",
    "    sum_num = sum_num + value\n",
    "sum_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算状态转移概率\n",
    "p_ss = dict_label['ss']/sum_num\n",
    "p_sb = dict_label['sb']/sum_num\n",
    "p_bm = dict_label['bm']/sum_num\n",
    "p_be = dict_label['be']/sum_num\n",
    "p_mm = dict_label['mm']/sum_num\n",
    "p_me = dict_label['me']/sum_num\n",
    "p_es = dict_label['es']/sum_num\n",
    "p_eb = dict_label['eb']/sum_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 维特比算法，维特比算法是一种动态规划算法用于寻找最有可能产生观测事件序列的-维特比路径\n",
    "\n",
    "# tag = pd.Series({'s':0, 'b':1, 'm':2, 'e':3, 'x':4})\n",
    "\n",
    "# 00 = ss = 1\n",
    "# 01 = sb = 1\n",
    "# 02 = sm = 0\n",
    "# 03 = se = 0\n",
    "# 10 = bs = 0\n",
    "# 11 = bb = 0\n",
    "# 12 = bm = 1\n",
    "# 13 = be = 1\n",
    "# 20 = ms = 0\n",
    "# 21 = mb = 0\n",
    "# 22 = mm = 1\n",
    "# 23 = me = 1\n",
    "# 30 = es = 1\n",
    "# 31 = eb = 1\n",
    "# 32 = em = 0\n",
    "# 33 = ee = 0\n",
    "\n",
    "# 定义状态转移矩阵\n",
    "transfer = [[p_ss,p_sb,0,0],\n",
    "            [0,0,p_bm,p_be],\n",
    "            [0,0,p_mm,p_me],\n",
    "            [p_es,p_eb,0,0]]\n",
    "\n",
    "# # 定义状态转移矩阵\n",
    "# transfer = [[1,1,0,0],\n",
    "#             [0,0,1,1],\n",
    "#             [0,0,1,1],\n",
    "#             [1,1,0,0]]\n",
    "\n",
    "# 根据符号断句\n",
    "cuts = re.compile(u'([\\da-zA-Z ]+)|[。，、？！\\.\\?,!]')\n",
    "\n",
    "# 预测分词\n",
    "def predict(sentence):\n",
    "    \n",
    "    # 如果句子大于最大长度，只取maxlen个词\n",
    "    if len(sentence) > maxlen:\n",
    "        sentence = sentence[:maxlen]\n",
    "    \n",
    "    # 预测结果，先把句子编程编号的形式，如果出现生僻字就填充0，然后给句子补0直到maxlen的长度。预测得到的结果只保留跟句子有效数据相同的长度\n",
    "    result = model.predict(np.array([list(chars[list(sentence)].fillna(0).astype(int))+[0]*(maxlen-len(sentence))]))[0][:len(sentence)]\n",
    "\n",
    "    # 存放最终结果\n",
    "    y = []\n",
    "    # 存放临时概率值\n",
    "    prob = []\n",
    "    # 计算最大转移概率\n",
    "    # 首先计算第1个词和第2个词,统计16种情况的概率\n",
    "    # result[0][j]第1个词的标签概率\n",
    "    # result[1][k]第2个词的标签概率\n",
    "    # transfer[j][k]对应的转移概率矩阵的概率\n",
    "    for j in range(4):\n",
    "        for k in range(4):\n",
    "            prob.append(result[0][j]*result[1][k]*transfer[j][k])\n",
    "    # 计算前一个词的的标签\n",
    "    word1 = np.argmax(prob)//4\n",
    "    # 计算后一个词的标签\n",
    "    word2 = np.argmax(prob)%4\n",
    "    # 保存结果\n",
    "    y.append(word1)\n",
    "    y.append(word2)\n",
    "    # 从第2个词开始\n",
    "    for i in range(1,len(sentence)-1):\n",
    "        # 存放临时概率值\n",
    "        prob = []\n",
    "        # 计算前一个词后后一个词的所有转移概率\n",
    "        for j in range(4):\n",
    "            prob.append(result[i][word2]*result[i+1][j]*transfer[word2][j])\n",
    "        # 计算后一个词的标签\n",
    "        word2 = np.argmax(prob)%4\n",
    "        # 保存结果\n",
    "        y.append(word2)\n",
    "        \n",
    "    # 分词\n",
    "    words = []\n",
    "    for i in range(len(sentence)):\n",
    "        # 如果标签为s或b，append到结果的list中\n",
    "        if y[i] in [0, 1]:\n",
    "            words.append(sentence[i])\n",
    "        else:\n",
    "        # 如果标签为m或e，在list最后一个元素中追加内容\n",
    "            words[-1] += sentence[i]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py:705: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['人们',\n",
       " '常说',\n",
       " '生活',\n",
       " '是',\n",
       " '一',\n",
       " '部',\n",
       " '教科',\n",
       " '书。',\n",
       " '人们',\n",
       " '常说',\n",
       " '生活',\n",
       " '是',\n",
       " '一',\n",
       " '部',\n",
       " '教科书',\n",
       " '。']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('人们常说生活是一部教科书。人们常说生活是一部教科书。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "def cut_word(s):\n",
    "    result = []\n",
    "    # 指针设置为0\n",
    "    j = 0\n",
    "    # 根据符号断句\n",
    "    for i in cuts.finditer(s):\n",
    "        # 对符号前的部分分词\n",
    "        result.extend(predict(s[j:i.start()]))\n",
    "        # 加入符号\n",
    "        result.append(s[i.start():i.end()])\n",
    "        # 移动指针到符号后面\n",
    "        j = i.end()\n",
    "    # 对最后的部分进行分词\n",
    "    result.extend(predict(s[j:]))\n",
    "#     if s[j:]:\n",
    "#         result.extend(predict(s[j:]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-df59f7f86a78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcut_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'人们常说生活是一部教科书。人们常说生活是一部教科书。'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-c1ea05133ae9>\u001b[0m in \u001b[0;36mcut_word\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# 对最后的部分进行分词\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m#     if s[j:]:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#         result.extend(predict(s[j:]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-4c842d37ce87>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0mprob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtransfer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;31m# 计算前一个词的的标签\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mword1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "cut_word('人们常说生活是一部教科书。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 13\n",
      "16 17\n",
      "17 18\n"
     ]
    }
   ],
   "source": [
    "for i in cuts.finditer('人们常说生活是一部教科书。大幅度。。发顺丰'):\n",
    "    print(i.start(),i.end())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "def cut_word1(s):\n",
    "    result = []\n",
    "    # 指针设置为0\n",
    "    j = 0\n",
    "    # 根据符号断句\n",
    "    for i in cuts.finditer(s):\n",
    "        # 对符号前的部分分词\n",
    "        result.extend(predict(s[j:i.start()]))\n",
    "        # 加入符号\n",
    "        result.append(s[i.start():i.end()])\n",
    "        # 移动指针到符号后面\n",
    "        j = i.end()\n",
    "    # 对最后的部分进行分词\n",
    "    if s[j:]:\n",
    "        result.extend(predict(s[j:]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['人们', '常说', '生活', '是', '一', '部', '教科书', '。']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_word1('人们常说生活是一部教科书。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 128)           660864    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 64)            98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 32, 5)             325       \n",
      "=================================================================\n",
      "Total params: 760,005\n",
      "Trainable params: 760,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 32, 128)           660864    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 32, 64)            98816     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 32, 5)             325       \n",
      "=================================================================\n",
      "Total params: 760,005\n",
      "Trainable params: 760,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
