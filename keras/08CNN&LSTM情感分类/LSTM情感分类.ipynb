{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kerashw08\n",
    "同样的数据，用lstm来完成情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ian\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import jieba # pip install jieba\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "neg=pd.read_excel('data/neg.xls',header=None)\n",
    "pos=pd.read_excel('data/pos.xls',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10428\n",
      "10677\n"
     ]
    }
   ],
   "source": [
    "#合并语料\n",
    "pn = pd.concat([pos,neg],ignore_index=True) \n",
    "#计算语料数目\n",
    "neglen = len(neg)\n",
    "print(neglen)\n",
    "poslen = len(pos) \n",
    "print(poslen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Ian\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.881 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...</td>\n",
       "      <td>[做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...   \n",
       "\n",
       "                                               words  \n",
       "0  [做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义分词函数\n",
    "cw = lambda x: list(jieba.cut(x))\n",
    "pn['words'] = pn[0].apply(cw)\n",
    "pn.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1804"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一行数据最多的词汇数\n",
    "max_document_length = max([len(x) for x in pn['words']])\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一个评论最多1000个词\n",
    "max_document_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(x) for x in pn['words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化分词器，设置字典中最大词汇数为30000\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "# 传入我们的训练数据，建立词典\n",
    "tokenizer.fit_on_texts(texts) \n",
    "# 把词转换为编号，词的编号根据词频设定，频率越大，编号越小\n",
    "sequences = tokenizer.texts_to_sequences(texts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[130,\n",
       "  503,\n",
       "  241,\n",
       "  40,\n",
       "  12,\n",
       "  12468,\n",
       "  83,\n",
       "  2,\n",
       "  1599,\n",
       "  1,\n",
       "  922,\n",
       "  210,\n",
       "  335,\n",
       "  1,\n",
       "  922,\n",
       "  210,\n",
       "  2309,\n",
       "  1,\n",
       "  922,\n",
       "  210,\n",
       "  39,\n",
       "  36,\n",
       "  529,\n",
       "  1,\n",
       "  37,\n",
       "  36,\n",
       "  1566,\n",
       "  3192,\n",
       "  1679,\n",
       "  2,\n",
       "  1017,\n",
       "  3,\n",
       "  6,\n",
       "  84,\n",
       "  1,\n",
       "  15,\n",
       "  5,\n",
       "  49,\n",
       "  60,\n",
       "  8,\n",
       "  20,\n",
       "  2,\n",
       "  17,\n",
       "  63,\n",
       "  1415,\n",
       "  2,\n",
       "  29,\n",
       "  530,\n",
       "  3436,\n",
       "  3,\n",
       "  168,\n",
       "  12468,\n",
       "  2,\n",
       "  648,\n",
       "  1,\n",
       "  5204,\n",
       "  37,\n",
       "  6,\n",
       "  119,\n",
       "  29,\n",
       "  669,\n",
       "  2,\n",
       "  10949,\n",
       "  2,\n",
       "  3193,\n",
       "  1,\n",
       "  49,\n",
       "  1600,\n",
       "  1136,\n",
       "  10,\n",
       "  17,\n",
       "  63,\n",
       "  699,\n",
       "  2,\n",
       "  4815,\n",
       "  1,\n",
       "  39,\n",
       "  63,\n",
       "  2719,\n",
       "  98,\n",
       "  29,\n",
       "  937,\n",
       "  199,\n",
       "  17,\n",
       "  1118,\n",
       "  2,\n",
       "  164,\n",
       "  244,\n",
       "  3,\n",
       "  8,\n",
       "  43,\n",
       "  12468,\n",
       "  10,\n",
       "  4313,\n",
       "  8991,\n",
       "  2,\n",
       "  130,\n",
       "  503,\n",
       "  2,\n",
       "  521,\n",
       "  87,\n",
       "  1,\n",
       "  37,\n",
       "  23,\n",
       "  414,\n",
       "  8260,\n",
       "  1,\n",
       "  503,\n",
       "  17,\n",
       "  4816,\n",
       "  766,\n",
       "  1083,\n",
       "  9,\n",
       "  5,\n",
       "  307,\n",
       "  8992,\n",
       "  1,\n",
       "  21850,\n",
       "  21851,\n",
       "  1095,\n",
       "  5003,\n",
       "  4,\n",
       "  1,\n",
       "  8993,\n",
       "  21852,\n",
       "  80,\n",
       "  113,\n",
       "  12,\n",
       "  4314,\n",
       "  3,\n",
       "  112,\n",
       "  1,\n",
       "  130,\n",
       "  503,\n",
       "  2,\n",
       "  116,\n",
       "  21853,\n",
       "  4,\n",
       "  1,\n",
       "  546,\n",
       "  816,\n",
       "  546,\n",
       "  1472,\n",
       "  4049,\n",
       "  53,\n",
       "  21854,\n",
       "  1,\n",
       "  17330,\n",
       "  1,\n",
       "  335,\n",
       "  3347,\n",
       "  3,\n",
       "  3826,\n",
       "  1,\n",
       "  145,\n",
       "  5,\n",
       "  9914,\n",
       "  3]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把序列设定为1000的长度，超过1000的部分舍弃，不到1000则补0\n",
    "sequences = pad_sequences(sequences, maxlen=1000, padding='post')  \n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21105, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词对应编号的字典\n",
    "dict_text = tokenizer.word_index\n",
    "dict_text['也']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义标签\n",
    "positive_labels = [1 for _ in range(poslen)]\n",
    "negative_labels = [0 for _ in range(neglen)]\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "# 打乱数据\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = sequences[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# 数据集切分为两部分\n",
    "test_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_10 (Embedding)     (None, 1000, 128)         3840000   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 1000, 64)          49408     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 64000)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 64001     \n",
      "=================================================================\n",
      "Total params: 3,953,409\n",
      "Trainable params: 3,953,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 定义函数式模型\n",
    "# 模型输入\n",
    "sequence_input = Input(shape=(1000,))\n",
    "# Embedding层，30000表示30000个词，每个词对应的向量为128维，序列长度为1000\n",
    "embedding_layer = Embedding(30000,\n",
    "                            128,\n",
    "                            input_length=1000)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "lstm = LSTM(64, return_sequences=True)(embedded_sequences)\n",
    "\n",
    "x = Flatten()(lstm)\n",
    "# 输出层\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "# 定义模型\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18995 samples, validate on 2110 samples\n",
      "Epoch 1/5\n",
      "18995/18995 [==============================] - 228s 12ms/step - loss: 0.3670 - acc: 0.8235 - val_loss: 0.2258 - val_acc: 0.9104\n",
      "Epoch 2/5\n",
      "18995/18995 [==============================] - 227s 12ms/step - loss: 0.1314 - acc: 0.9556 - val_loss: 0.1958 - val_acc: 0.9284\n",
      "Epoch 3/5\n",
      "18995/18995 [==============================] - 224s 12ms/step - loss: 0.0699 - acc: 0.9770 - val_loss: 0.1984 - val_acc: 0.9318\n",
      "Epoch 4/5\n",
      "18995/18995 [==============================] - 226s 12ms/step - loss: 0.0436 - acc: 0.9878 - val_loss: 0.2796 - val_acc: 0.9284\n",
      "Epoch 5/5\n",
      "18995/18995 [==============================] - 226s 12ms/step - loss: 0.0311 - acc: 0.9917 - val_loss: 0.2460 - val_acc: 0.9275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2feab43b080>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 采用bilstm\n",
    "效果应该是最好的，不过最后发生过拟合了！\n",
    "![](bilstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_11 (Embedding)     (None, 1000, 128)         3840000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 128)         98816     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 128001    \n",
      "=================================================================\n",
      "Total params: 4,066,817\n",
      "Trainable params: 4,066,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.wrappers import Bidirectional\n",
    "# 定义函数式模型\n",
    "# 模型输入\n",
    "sequence_input = Input(shape=(1000,))\n",
    "# Embedding层，30000表示30000个词，每个词对应的向量为128维，序列长度为1000\n",
    "embedding_layer = Embedding(30000,\n",
    "                            128,\n",
    "                            input_length=1000)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "lstm = Bidirectional(LSTM(64, return_sequences=True))(embedded_sequences)\n",
    "\n",
    "x = Flatten()(lstm)\n",
    "# 输出层\n",
    "preds = Dense(1, activation='sigmoid')(x)\n",
    "# 定义模型\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18995 samples, validate on 2110 samples\n",
      "Epoch 1/5\n",
      "18995/18995 [==============================] - 514s 27ms/step - loss: 0.3768 - acc: 0.8204 - val_loss: 0.1980 - val_acc: 0.9299\n",
      "Epoch 2/5\n",
      "18995/18995 [==============================] - 511s 27ms/step - loss: 0.1259 - acc: 0.9551 - val_loss: 0.1808 - val_acc: 0.9370\n",
      "Epoch 3/5\n",
      "18995/18995 [==============================] - 513s 27ms/step - loss: 0.0676 - acc: 0.9777 - val_loss: 0.2160 - val_acc: 0.9336\n",
      "Epoch 4/5\n",
      "18995/18995 [==============================] - 513s 27ms/step - loss: 0.0394 - acc: 0.9880 - val_loss: 0.2790 - val_acc: 0.9284\n",
      "Epoch 5/5\n",
      "18995/18995 [==============================] - 519s 27ms/step - loss: 0.0263 - acc: 0.9939 - val_loss: 0.3060 - val_acc: 0.9251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2fea9182ac8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
